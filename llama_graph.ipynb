{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8f29df9-cd95-4ca7-a950-ef70d86ddf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8892714-02af-4403-b0e0-385d43736fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open dataset and store it as a JSON object\n",
    "with open('../GSM-IC/GSM-IC_mstep.json', 'r') as file:\n",
    "    questions = json.load(file)\n",
    "with open('../GSM-IC/GSM-IC_2step.json', 'r') as file:\n",
    "    questions2 = json.load(file)\n",
    "    #initialize the Groq client. This allows us to query Llama3 (or another model) from my Groq account\n",
    "#todo: store the api_key in a secure way!\n",
    "client = Groq(\n",
    "    # api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "    api_key='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'\n",
    ")\n",
    "# client = OpenAI(api_key=open(\"api.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c64d8681-44a7-43b3-9225-4acaa1f2b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: 1. answer_prompt is the current way of forcing the model to output its answer in predictable format. Find a better way to extract the numerical answer from the model's output. (maybe: require JSON output)\n",
    "\n",
    "#todo: 2. clean up the answer. Ex. remove $, %, inches, etc. \n",
    "\n",
    "def single_query(query):\n",
    "    \"\"\"\n",
    "    query the model with a single question\n",
    "    input: what you would type in the chat box to ask the model a question\n",
    "    output: the model's output in response to your query \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    answer_prompt='The last sentence of your response must be in the form \"The answer is [your answer]\".'\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query + answer_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ee02e04-db86-473b-bceb-84facfdc5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: shouldn't need to separate the two queries like this\n",
    "def multiple_queries(question_type='original_question', num_questions=5):\n",
    "    \"\"\"\n",
    "    query the model sequentially with multiple questions from the dataset \n",
    "    Args: question_type: specify whether to query the model with the original question or the new question \n",
    "        num_questions: the number of questions (mainly for testing a few questions at a time)\n",
    "    \n",
    "    \"\"\"\n",
    "    for idx, question in enumerate(questions):\n",
    "        if idx < num_questions:\n",
    "            query = question[question_type]\n",
    "            response = single_query(query)\n",
    "            print(response)\n",
    "            print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34bbabb7-feef-416a-a115-2b03cb531405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "#todo: shouldn't need to separate the two queries like this\n",
    "def get_answers(questions, question_type='original_question', num_questions=5):\n",
    "    \"\"\"\n",
    "    query the model sequentially with multiple questions from the dataset \n",
    "    Args: question_type: specify whether to query the model with the original question or the new question \n",
    "        num_questions: the number of questions (mainly for testing a few questions at a time)\n",
    "    \n",
    "    \"\"\"\n",
    "    answers = {}\n",
    "    for idx, question in tqdm.tqdm(list(enumerate(questions[:num_questions]))):\n",
    "        if idx < num_questions:\n",
    "            query = question[question_type]\n",
    "            response = single_query(query)\n",
    "            # print(response)\n",
    "            try:\n",
    "                answer = response.split(\"he answer is\")[1]#.strip(\"$\")\n",
    "                # print(question['answer'], answer, question)\n",
    "                answers[idx] = [question['answer'] in answer, answer]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                # pass\n",
    "                # print(response, question['answer'])\n",
    "                # input()\n",
    "        else:\n",
    "            break\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4757b55-5165-4ffe-aecd-4400d847993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▉                                  | 94/500 [05:57<15:00,  2.22s/it]"
     ]
    }
   ],
   "source": [
    "answers = get_answers(questions2, question_type='new_question', num_questions=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16ac6f-9e8a-4f0e-850f-abed487e19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_steps = []\n",
    "for steps in [3, 4, 5, 6, 7]:\n",
    "    answers_steps.append(get_answers([i for i in questions if i['n_steps'] == steps], question_type='new_question', num_questions=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b24b4d-f58e-4f1b-a8d7-da37b0a35709",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [questions2]\n",
    "for steps in [3, 4, 5, 6, 7]:\n",
    "    x.append([i for i in questions if i['n_steps'] == steps])\n",
    "y = [answers]\n",
    "y.extend(answers_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce9e1b-37ea-4109-89a5-95d8ce183388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f=open(\"answers_llama_3_1_9b.obj\", \"wb\")\n",
    "pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f65fc-9493-4c2d-beab-98e1bfed18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_accs = [[0, 0] for _ in range(10)]\n",
    "for q, a in zip(x, y):\n",
    "    for k, v in a.items():\n",
    "        question = q[k]\n",
    "        step_accs[question['n_steps']][0] += int(v[0])\n",
    "        step_accs[question['n_steps']][1] += 1\n",
    "\n",
    "print(step_accs)\n",
    "step_accs = [i[0] / i[1] for i in step_accs[2:8]]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.title(\"Llama3-8b Accuracy\")\n",
    "plt.bar(list(range(2, 2+len(step_accs))), step_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
