{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "661b1e98-2e88-427c-96d2-e5f58473aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b029bbd1-63b8-4f0a-bbb9-58d4bce7fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open dataset and store it as a JSON object\n",
    "with open('../GSM-IC/GSM-IC_mstep.json', 'r') as file:\n",
    "    questions = json.load(file)\n",
    "with open('../GSM-IC/GSM-IC_2step.json', 'r') as file:\n",
    "    questions2 = json.load(file)\n",
    "    #initialize the Groq client. This allows us to query Llama3 (or another model) from my Groq account\n",
    "#todo: store the api_key in a secure way!\n",
    "# client = Groq(\n",
    "    # api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "    # api_key='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'\n",
    "# )\n",
    "client = OpenAI(api_key=open(\"api.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c911366-dff7-4a87-bf5e-b1cdef5b0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: 1. answer_prompt is the current way of forcing the model to output its answer in predictable format. Find a better way to extract the numerical answer from the model's output. (maybe: require JSON output)\n",
    "\n",
    "#todo: 2. clean up the answer. Ex. remove $, %, inches, etc. \n",
    "\n",
    "def single_query(query):\n",
    "    \"\"\"\n",
    "    query the model with a single question\n",
    "    input: what you would type in the chat box to ask the model a question\n",
    "    output: the model's output in response to your query \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    answer_prompt='The last sentence of your response must be in the form \"The answer is [your answer]\".'\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query + answer_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95813338-fea6-4253-9b84-bc13687e56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "s7 = [i for i in questions if i['n_steps'] == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e3a492f-84de-48a6-b176-5794550ae46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"thoughts\": \"Let\\'s break down the problem step by step. In the first round, Sam skipped 16 times and Jeff skipped 15 times. In the second round, Sam skipped 16 times and Jeff skipped 13 times. In the third round, Sam skipped 16 times and Jeff skipped 20 times. In the last round, Sam skipped 16 times and Jeff skipped 8 times. So, the total number of skips for Jeff is 15 + 13 + 20 + 8 = 56. To find the average number of skips per round, we divide the total number of skips by the number of rounds, which is 4. So, the answer is 56 / 4 = 14.\",\\n\"answer\": 14,\\n\"irrelevant\": false\\n}'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_query(s7[0]['new_question'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
